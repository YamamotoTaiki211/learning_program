# dashboard_enhanced.py - ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã¨æœˆæ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚’è¿½åŠ ã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³
import io
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st
from datetime import datetime

# ===== Page config =====
st.set_page_config(page_title="Sales BI Enhanced", layout="wide")

# ===== Constants =====
DATA_PATH = "data/sample_sales.csv"
COLS = {
    "date": "date",
    "category": "category",
    "units": "units",
    "unit_price": "unit_price",
    "region": "region",
    "sales_channel": "sales_channel",
    "customer_segment": "customer_segment",
    "revenue": "revenue",
}

# ===== Data Load (cached) =====
@st.cache_data
def load_data(path: str) -> pd.DataFrame:
    df = pd.read_csv(
        path,
        dtype={
            COLS["category"]: "string",
            COLS["region"]: "string",
            COLS["sales_channel"]: "string",
            COLS["customer_segment"]: "string",
        },
        parse_dates=[COLS["date"]],
        encoding="utf-8",
    )
    # Cast to category for performance
    for c in [COLS["category"], COLS["region"], COLS["sales_channel"], COLS["customer_segment"]]:
        df[c] = df[c].astype("category")

    # Numeric coercion
    for c in [COLS["units"], COLS["unit_price"], COLS["revenue"]]:
        df[c] = pd.to_numeric(df[c], errors="coerce")

    # Simple validation
    missing = [c for c in COLS.values() if c not in df.columns]
    if missing:
        raise ValueError(f"æƒ³å®šã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing}")

    return df

try:
    df = load_data(DATA_PATH)
except Exception as e:
    st.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    st.stop()

st.title("å£²ä¸ŠBIãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆå¼·åŒ–ç‰ˆï¼‰")

# ===== Sidebar Filters =====
with st.sidebar:
    st.header("ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")

    # Date range from data min/max
    min_date = pd.to_datetime(df[COLS["date"]].min())
    max_date = pd.to_datetime(df[COLS["date"]].max())
    date_range = st.date_input(
        "æœŸé–“",
        value=(min_date.date(), max_date.date()),
        min_value=min_date.date(),
        max_value=max_date.date(),
        format="YYYY-MM-DD",
    )

    cats = st.multiselect("ã‚«ãƒ†ã‚´ãƒª", options=sorted(df[COLS["category"]].cat.categories.tolist()))
    regs = st.multiselect("åœ°åŸŸ", options=sorted(df[COLS["region"]].cat.categories.tolist()))
    chans = st.multiselect("è²©å£²ãƒãƒ£ãƒãƒ«", options=sorted(df[COLS["sales_channel"]].cat.categories.tolist()))
    segs = st.multiselect("é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ", options=sorted(df[COLS["customer_segment"]].cat.categories.tolist()))

    reset = st.button("ãƒ•ã‚£ãƒ«ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ")

if reset:
    # Rerun to clear widget states
    st.session_state.clear()
    st.rerun()

# ===== Filtering (pure) =====
start_dt = pd.to_datetime(date_range[0])
end_dt = pd.to_datetime(date_range[1])
mask = (
    df[COLS["date"]].between(start_dt, end_dt)
    & (df[COLS["category"]].isin(cats) if cats else True)
    & (df[COLS["region"]].isin(regs) if regs else True)
    & (df[COLS["sales_channel"]].isin(chans) if chans else True)
    & (df[COLS["customer_segment"]].isin(segs) if segs else True)
)
fdf = df.loc[mask].copy()

if fdf.empty:
    st.info("è©²å½“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã‚’ç·©ã‚ã¦ãã ã•ã„ã€‚")
    st.stop()

# ===== KPI =====
total_revenue = float(fdf[COLS["revenue"]].sum())
total_units = float(fdf[COLS["units"]].sum())
avg_price = (total_revenue / total_units) if total_units else np.nan
records = int(len(fdf))

# Optional quality check
fdf["calc_revenue"] = fdf[COLS["units"]] * fdf[COLS["unit_price"]]
diff = fdf[COLS["revenue"]] - fdf["calc_revenue"]
mismatch_cnt = int((diff.fillna(0) != 0).sum())
max_dev = float((diff.abs() / fdf[COLS["revenue"]].replace(0, np.nan)).max())

col1, col2, col3, col4, col5, col6 = st.columns(6)
col1.metric("åˆè¨ˆå£²ä¸Š(å††)", f"{total_revenue:,.0f}")
col2.metric("åˆè¨ˆæ•°é‡", f"{total_units:,.0f}")
col3.metric("å¹³å‡å˜ä¾¡(å††)", "-" if np.isnan(avg_price) else f"{avg_price:,.0f}")
col4.metric("ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", f"{records:,}")
col5.metric("ä¹–é›¢ä»¶æ•°", f"{mismatch_cnt:,}")
col6.metric("æœ€å¤§ä¹–é›¢ç‡", "-" if np.isinf(max_dev) or np.isnan(max_dev) else f"{max_dev*100:,.2f}%")

st.markdown("---")

# ===== Charts =====
# 1) æ—¥æ¬¡å£²ä¸Šæ¨ç§»
ts = (
    fdf.groupby(COLS["date"], as_index=False)[COLS["revenue"]]
    .sum()
    .sort_values(COLS["date"])
)
fig_ts = px.line(
    ts, x=COLS["date"], y=COLS["revenue"],
    title="æ—¥æ¬¡å£²ä¸Šæ¨ç§»", labels={COLS["date"]: "æ—¥ä»˜", COLS["revenue"]: "å£²ä¸Š(å††)"}
)
st.plotly_chart(fig_ts, use_container_width=True)

# åˆ†æè§£é‡ˆ
if len(ts) > 1:
    max_day = ts.loc[ts[COLS["revenue"]].idxmax()]
    min_day = ts.loc[ts[COLS["revenue"]].idxmin()]
    avg_daily = ts[COLS["revenue"]].mean()
    trend = "ä¸Šæ˜‡" if ts[COLS["revenue"]].iloc[-1] > ts[COLS["revenue"]].iloc[0] else "ä¸‹é™"
    
    st.info(f"""
    ğŸ“Š **æ—¥æ¬¡å£²ä¸Šæ¨ç§»ã®åˆ†æ:**
    - æœ€é«˜å£²ä¸Šæ—¥: {max_day[COLS['date']].strftime('%Y-%m-%d')} ({max_day[COLS['revenue']]:,.0f}å††)
    - æœ€ä½å£²ä¸Šæ—¥: {min_day[COLS['date']].strftime('%Y-%m-%d')} ({min_day[COLS['revenue']]:,.0f}å††)
    - å¹³å‡æ—¥æ¬¡å£²ä¸Š: {avg_daily:,.0f}å††
    - æœŸé–“å…¨ä½“ã®ãƒˆãƒ¬ãƒ³ãƒ‰: {trend}å‚¾å‘
    """)
else:
    st.info("ğŸ“Š **æ—¥æ¬¡å£²ä¸Šæ¨ç§»:** ãƒ‡ãƒ¼ã‚¿ãŒ1æ—¥åˆ†ã®ã¿ã®ãŸã‚ã€ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã¯ã§ãã¾ã›ã‚“ã€‚")

# 2) ã‚«ãƒ†ã‚´ãƒªåˆ¥å£²ä¸Š
cat_rev = (
    fdf.groupby(COLS["category"], as_index=False)[COLS["revenue"]]
    .sum()
    .sort_values(COLS["revenue"], ascending=False)
)
fig_cat = px.bar(
    cat_rev, x=COLS["category"], y=COLS["revenue"],
    title="ã‚«ãƒ†ã‚´ãƒªåˆ¥å£²ä¸Š", labels={COLS["category"]: "ã‚«ãƒ†ã‚´ãƒª", COLS["revenue"]: "å£²ä¸Š(å††)"}
)
st.plotly_chart(fig_cat, use_container_width=True)

# åˆ†æè§£é‡ˆ
if len(cat_rev) > 0:
    top_cat = cat_rev.iloc[0]
    total_cat_revenue = cat_rev[COLS["revenue"]].sum()
    top_cat_share = (top_cat[COLS["revenue"]] / total_cat_revenue * 100)
    
    analysis_text = f"""
    ğŸ“Š **ã‚«ãƒ†ã‚´ãƒªåˆ¥å£²ä¸Šã®åˆ†æ:**
    - æœ€é«˜å£²ä¸Šã‚«ãƒ†ã‚´ãƒª: **{top_cat[COLS['category']]}** ({top_cat[COLS['revenue']]:,.0f}å††, {top_cat_share:.1f}%)
    - ã‚«ãƒ†ã‚´ãƒªæ•°: {len(cat_rev)}ç¨®é¡
    """
    
    if len(cat_rev) > 1:
        second_cat = cat_rev.iloc[1]
        analysis_text += f"- 2ä½: **{second_cat[COLS['category']]}** ({second_cat[COLS['revenue']]:,.0f}å††)"
    
    st.info(analysis_text)

# 3) æ–°æ©Ÿèƒ½: æœˆæ¬¡å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
fdf_monthly = fdf.copy()
fdf_monthly['å¹´æœˆ'] = fdf_monthly[COLS["date"]].dt.to_period('M').astype(str)
monthly_trend = (
    fdf_monthly.groupby(['å¹´æœˆ', COLS["region"]], as_index=False)[COLS["revenue"]]
    .sum()
    .sort_values('å¹´æœˆ')
)
fig_monthly = px.line(
    monthly_trend, x='å¹´æœˆ', y=COLS["revenue"], color=COLS["region"],
    title="æœˆæ¬¡å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆåœ°åŸŸåˆ¥ï¼‰", 
    labels={'å¹´æœˆ': 'å¹´æœˆ', COLS["revenue"]: 'å£²ä¸Š(å††)', COLS["region"]: 'åœ°åŸŸ'}
)
fig_monthly.update_xaxes(tickangle=45)
st.plotly_chart(fig_monthly, use_container_width=True)

# åˆ†æè§£é‡ˆ
if len(monthly_trend) > 0:
    regions_in_data = monthly_trend[COLS["region"]].unique()
    best_region_month = monthly_trend.loc[monthly_trend[COLS["revenue"]].idxmax()]
    
    # åœ°åŸŸåˆ¥ã®æœˆæ¬¡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
    region_performance = monthly_trend.groupby(COLS["region"])[COLS["revenue"]].agg(['sum', 'mean', 'std']).round(0)
    best_region = region_performance['sum'].idxmax()
    
    analysis_text = f"""
    ğŸ“Š **æœˆæ¬¡å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰ã®åˆ†æ:**
    - æœ€é«˜å£²ä¸Šæœˆ: **{best_region_month['å¹´æœˆ']}** ã® {best_region_month[COLS['region']]} ({best_region_month[COLS['revenue']]:,.0f}å††)
    - æœ€é«˜å£²ä¸Šåœ°åŸŸ: **{best_region}** (ç´¯è¨ˆ: {region_performance.loc[best_region, 'sum']:,.0f}å††)
    - å¯¾è±¡åœ°åŸŸæ•°: {len(regions_in_data)}åœ°åŸŸ
    """
    
    if len(region_performance) > 1:
        most_stable = region_performance['std'].idxmin()
        analysis_text += f"- æœ€ã‚‚å®‰å®šã—ãŸåœ°åŸŸ: **{most_stable}** (æ¨™æº–åå·®: {region_performance.loc[most_stable, 'std']:,.0f})"
    
    st.info(analysis_text)

# 4) æ–°æ©Ÿèƒ½: ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆåœ°åŸŸÃ—æ›œæ—¥ã®å£²ä¸Šå¯†åº¦ï¼‰
fdf_heatmap = fdf.copy()
fdf_heatmap['æ›œæ—¥'] = fdf_heatmap[COLS["date"]].dt.day_name()
heatmap_data = (
    fdf_heatmap.groupby([COLS["region"], 'æ›œæ—¥'], as_index=False)[COLS["revenue"]]
    .sum()
    .pivot(index=COLS["region"], columns='æ›œæ—¥', values=COLS["revenue"])
    .fillna(0)
)

# æ›œæ—¥ã®é †åºã‚’èª¿æ•´
weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
heatmap_data = heatmap_data.reindex(columns=[day for day in weekday_order if day in heatmap_data.columns])

fig_heatmap = go.Figure(data=go.Heatmap(
    z=heatmap_data.values,
    x=heatmap_data.columns,
    y=heatmap_data.index,
    colorscale='Blues',
    text=heatmap_data.values,
    texttemplate='%{text:,.0f}',
    textfont={"size": 10},
    hoverongaps=False
))
fig_heatmap.update_layout(
    title="å£²ä¸Šãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆåœ°åŸŸÃ—æ›œæ—¥ï¼‰",
    xaxis_title="æ›œæ—¥",
    yaxis_title="åœ°åŸŸ"
)
st.plotly_chart(fig_heatmap, use_container_width=True)

# åˆ†æè§£é‡ˆ
if not heatmap_data.empty:
    # æœ€é«˜å£²ä¸Šã®çµ„ã¿åˆã‚ã›ã‚’è¦‹ã¤ã‘ã‚‹
    max_value = heatmap_data.values.max()
    max_pos = np.unravel_index(heatmap_data.values.argmax(), heatmap_data.shape)
    best_region = heatmap_data.index[max_pos[0]]
    best_day = heatmap_data.columns[max_pos[1]]
    
    # å„åœ°åŸŸã®æœ€ã‚‚å£²ä¸ŠãŒé«˜ã„æ›œæ—¥
    best_days_by_region = heatmap_data.idxmax(axis=1)
    
    # å„æ›œæ—¥ã®å£²ä¸Šåˆè¨ˆ
    daily_totals = heatmap_data.sum(axis=0).sort_values(ascending=False)
    best_overall_day = daily_totals.index[0]
    
    analysis_text = f"""
    ğŸ“Š **å£²ä¸Šãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®åˆ†æ:**
    - æœ€é«˜å£²ä¸Šã®çµ„ã¿åˆã‚ã›: **{best_region}** ã® **{best_day}** ({max_value:,.0f}å††)
    - å…¨ä½“ã§æœ€ã‚‚å£²ä¸ŠãŒé«˜ã„æ›œæ—¥: **{best_overall_day}** ({daily_totals.iloc[0]:,.0f}å††)
    - åˆ†æå¯¾è±¡: {len(heatmap_data)}åœ°åŸŸ Ã— {len(heatmap_data.columns)}æ›œæ—¥
    """
    
    # åœ°åŸŸã”ã¨ã®æœ€é©æ›œæ—¥ã‚’è¡¨ç¤º
    if len(best_days_by_region) > 1:
        region_insights = []
        for region, day in best_days_by_region.items():
            revenue = heatmap_data.loc[region, day]
            if revenue > 0:
                region_insights.append(f"{region}: {day}")
        
        if region_insights:
            analysis_text += f"\n    - åœ°åŸŸåˆ¥æœ€é©æ›œæ—¥: {', '.join(region_insights[:3])}"
            if len(region_insights) > 3:
                analysis_text += f" ãªã©"
    
    st.info(analysis_text)

# 5) åœ°åŸŸÃ—ãƒãƒ£ãƒãƒ« ãƒ”ãƒœãƒƒãƒˆï¼ˆç©ã¿ä¸Šã’æ£’ï¼‰
pivot = (
    fdf.pivot_table(
        index=COLS["region"],
        columns=COLS["sales_channel"],
        values=COLS["revenue"],
        aggfunc="sum",
        fill_value=0,
    )
    .reset_index()
)
pivot_melt = pivot.melt(id_vars=COLS["region"], var_name="è²©å£²ãƒãƒ£ãƒãƒ«", value_name="å£²ä¸Š(å††)")
fig_pv = px.bar(
    pivot_melt, x=COLS["region"], y="å£²ä¸Š(å††)", color="è²©å£²ãƒãƒ£ãƒãƒ«",
    title="åœ°åŸŸÃ—è²©å£²ãƒãƒ£ãƒãƒ«ã®å£²ä¸Šï¼ˆç©ã¿ä¸Šã’ï¼‰", labels={COLS["region"]: "åœ°åŸŸ"}
)
st.plotly_chart(fig_pv, use_container_width=True)

# åˆ†æè§£é‡ˆ
if len(pivot_melt) > 0:
    # åœ°åŸŸåˆ¥ç·å£²ä¸Š
    region_totals = pivot_melt.groupby(COLS["region"])["å£²ä¸Š(å††)"].sum().sort_values(ascending=False)
    best_region_channel = region_totals.index[0]
    
    # ãƒãƒ£ãƒãƒ«åˆ¥ç·å£²ä¸Š
    channel_totals = pivot_melt.groupby("è²©å£²ãƒãƒ£ãƒãƒ«")["å£²ä¸Š(å††)"].sum().sort_values(ascending=False)
    best_channel = channel_totals.index[0]
    
    # æœ€é«˜å£²ä¸Šã®çµ„ã¿åˆã‚ã›
    best_combination = pivot_melt.loc[pivot_melt["å£²ä¸Š(å††)"].idxmax()]
    
    analysis_text = f"""
    ğŸ“Š **åœ°åŸŸÃ—è²©å£²ãƒãƒ£ãƒãƒ«ã®åˆ†æ:**
    - æœ€é«˜å£²ä¸Šåœ°åŸŸ: **{best_region_channel}** ({region_totals.iloc[0]:,.0f}å††)
    - æœ€é«˜å£²ä¸Šãƒãƒ£ãƒãƒ«: **{best_channel}** ({channel_totals.iloc[0]:,.0f}å††)
    - æœ€å¼·ã®çµ„ã¿åˆã‚ã›: **{best_combination[COLS['region']]}** Ã— **{best_combination['è²©å£²ãƒãƒ£ãƒãƒ«']}** ({best_combination['å£²ä¸Š(å††)']:,.0f}å††)
    - åˆ†æå¯¾è±¡: {len(region_totals)}åœ°åŸŸ Ã— {len(channel_totals)}ãƒãƒ£ãƒãƒ«
    """
    
    st.info(analysis_text)

st.markdown("---")

# ===== Data Table & Download =====
st.subheader("æ˜ç´°ï¼ˆç¾åœ¨ã®ãƒ•ã‚£ãƒ«ã‚¿åæ˜ ï¼‰")
st.dataframe(fdf.drop(columns=["calc_revenue"]), use_container_width=True)

csv_bytes = fdf.drop(columns=["calc_revenue"]).to_csv(index=False).encode("utf-8-sig")
st.download_button(
    "CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=csv_bytes,
    file_name=f"sales_filtered_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
    mime="text/csv",
)

# ===== Notes =====
with st.expander("å®Ÿè£…ãƒ¡ãƒ¢"):
    st.write(
        "**Enhanced Version è¿½åŠ æ©Ÿèƒ½:**\n"
        "- æœˆæ¬¡å£²ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆåœ°åŸŸåˆ¥ãƒ©ã‚¤ãƒ³ï¼‰\n"
        "- ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆåœ°åŸŸÃ—æ›œæ—¥ã®å£²ä¸Šå¯†åº¦ï¼‰\n\n"
        "**æ—¢å­˜æ©Ÿèƒ½:**\n"
        "- `revenue` ã¯çœŸå€¤ã¨ã—ã¦æ‰±ã„ã€å†è¨ˆç®—ã§ä¸Šæ›¸ãã—ãªã„\n"
        "- å¹³å‡å˜ä¾¡ã¯é›†è¨ˆå¾Œã« `sum(revenue) / sum(units)` ã§ç®—å‡º\n"
        "- ã‚«ãƒ†ã‚´ãƒªç­‰ã¯å‹•çš„ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ã§é¸æŠè‚¢ã‚’ç”Ÿæˆ\n"
        "- æ¬ æã¯ to_numeric(..., errors='coerce') ã§å¸å\n"
        "- ä¹–é›¢ãƒã‚§ãƒƒã‚¯ã¯å“è³ªç›£è¦–ã®å‚è€ƒï¼ˆä»»æ„ï¼‰"
    )